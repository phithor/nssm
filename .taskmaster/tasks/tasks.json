{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Python Project & Repository",
        "description": "Create git repo, Python virtual environment, and baseline directory structure for NSSM modules (scraper, db, nlp, dashboard).",
        "details": "• mkdir NSSM/{scraper,db,nlp,dashboard,config,tests}\n• git init; create .gitignore (venv, __pycache__, .env)\n• poetry init or pip-tools requirements.txt with python 3.11\n• Add core deps: requests, beautifulsoup4, psycopg2-binary, sqlalchemy, pandas, scikit-learn, transformers, streamlit, plotly, schedule, python-dotenv\n• Setup pre-commit hooks: black, isort, flake8\n• Dockerfile: slim-buster, copy src, install deps\n• docker-compose.yml with app + postgres 15.\n• Seed README explaining architecture.",
        "testStrategy": "Run ‘docker-compose up’ → container builds without errors. Execute ‘pytest -q’ (empty suite) returns 0. Verify pre-commit runs on git commit.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Git Repository & Seed README",
            "description": "Create project root, initialize git repository, and add a baseline README describing NSSM architecture.",
            "dependencies": [],
            "details": "- Create project directory `NSSM` and navigate into it.\n- Run `git init` to initialize repository.\n- Add initial `README.md` including project overview, module descriptions, and contribution guidelines.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Generate Python Packaging & Dependency Management",
            "description": "Set up Poetry with Python 3.11 and define core dependencies for the project.",
            "dependencies": [
              "1.1"
            ],
            "details": "- Run `poetry init --name NSSM --python ^3.11`.\n- Add dependencies: requests, beautifulsoup4, psycopg2-binary, sqlalchemy, pandas, scikit-learn, transformers, streamlit, plotly, schedule, python-dotenv.\n- Configure Poetry virtual environment and ensure `poetry.lock` is committed.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Establish Directory Structure with Module Stubs",
            "description": "Create baseline folder hierarchy and `__init__.py` files for scraper, db, nlp, dashboard, config, and tests.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "- Execute `mkdir -p NSSM/{scraper,db,nlp,dashboard,config,tests}`.\n- Add `__init__.py` to each package directory.\n- Include placeholder test file `tests/test_placeholder.py`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Configure Development Tooling",
            "description": "Add .gitignore, pre-commit hooks (black, isort, flake8), and pytest boilerplate for consistent development workflow.",
            "dependencies": [
              "1.1",
              "1.2",
              "1.3"
            ],
            "details": "- Create `.gitignore` covering venv, `__pycache__`, `.env`, and build artifacts.\n- Write `.flake8`, `pyproject.toml` or separate config for black & isort.\n- Install and configure `pre-commit`; add hooks for black, isort, flake8.\n- Add minimal `pytest.ini` and verify `pytest -q` returns 0.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add Containerization Assets & Verify Compose",
            "description": "Create Dockerfile and docker-compose.yml with Postgres service; ensure `docker-compose up` builds and runs successfully.",
            "dependencies": [
              "1.2",
              "1.3",
              "1.4"
            ],
            "details": "- Write `Dockerfile` based on python:3.11-slim-buster, copy source, and install dependencies via Poetry.\n- Create `docker-compose.yml` defining app service and Postgres 15 database.\n- Run `docker-compose up --build` and verify containers start without errors.\n<info added on 2025-08-27T12:41:16.608Z>\nCommitted containerization assets:\n• Dockerfile (python:3.11-slim-buster + Poetry)\n• docker-compose.yml (app service + Postgres 15 with dedicated network)\n• .dockerignore to slim build context (env files, venv, caches, docs, tests)\n\nCompose file passes `docker-compose config` lint check; full runtime verification deferred until Docker runtime is available on this workstation.\n</info added on 2025-08-27T12:41:16.608Z>",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Design & Migrate PostgreSQL Schema",
        "description": "Define relational schema for posts, tickers, sentiment, alerts and create migration script.",
        "details": "• Use SQLAlchemy declarative models in db/models.py\n• Tables: forums(id, name,url), posts(id, forum_id, ticker, timestamp, author, raw_text, clean_text, sentiment_score FLOAT), sentiment_agg(id,ticker,interval_start,interval_end,avg_score,post_cnt,created_at), alerts(id,ticker,rule,triggered_at)\n• Alembic revision to create tables.\n• Use TimescaleDB extension for sentiment_agg hypertable (optional enable if docker-compose env TS).",
        "testStrategy": "Run ‘alembic upgrade head’ in container. Connect via psql → \\dt lists all tables. Insert mock row → SELECT verifies constraints. pytest with in-memory postgres (pytest-postgresql) validating model round-trip.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up SQLAlchemy Base and Engine",
            "description": "Create the foundational database module with Base class and engine configuration.",
            "dependencies": [],
            "details": "Add db/__init__.py; define Base = declarative_base(); read DATABASE_URL from env; expose get_engine() and get_session() helpers; include basic logging.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Declarative Models",
            "description": "Define ORM classes for forums, posts, sentiment_agg, alerts, anomalies, and market tables.",
            "dependencies": [
              "2.1"
            ],
            "details": "Create db/models.py; add columns, primary/foreign keys, indexes, relationships, __repr__; enforce NOT NULL & unique constraints where appropriate.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Alembic and Generate Initial Migration",
            "description": "Initialize Alembic, link metadata, and create first revision to build core tables.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "Run `alembic init migrations`; update env.py with target_metadata = Base.metadata; generate revision 'create_core_tables'; test `alembic upgrade head` against local Postgres.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add TimescaleDB Extension & Hypertable Logic",
            "description": "Extend migration script to optionally enable TimescaleDB and convert sentiment_agg to hypertable.",
            "dependencies": [
              "2.3"
            ],
            "details": "Within upgrade(): check os.getenv('TS'); execute `CREATE EXTENSION IF NOT EXISTS timescaledb`; run `SELECT create_hypertable('sentiment_agg', 'interval_start', if_not_exists=>TRUE)`; include corresponding downgrade steps.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write Seeds, Fixtures, and Unit Tests",
            "description": "Create seed scripts and pytest fixtures to validate CRUD operations with the new schema.",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "Add db/seed.py populating mock forums and posts; configure pytest-postgresql fixture applying migrations; assert insert/select/update/delete for each model; include tox entry.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Update Docker Compose and Automate Migration",
            "description": "Modify docker-compose to include env vars, volumes, and run Alembic upgrade during container start.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "Add TIMESCALEDB flag, mount ./migrations; in app service CMD run `alembic upgrade head`; verify by `docker-compose up` then `psql -c '\\dt'`; document process in README.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Forum Scraper (Hegnar & Avanza)",
        "description": "Build resilient, polite scrapers that run continuously and store raw posts.",
        "details": "• scraper/base.py defines Scraper abstract class(fetch(),parse())\n• Implement hegnar.py, avanza.py using requests+bs4; fallback to Selenium for JS.\n• Respect robots.txt but randomize headers, add 3-8s sleep.\n• Extract (ticker regex like r\"\\b[A-Z]{2,4}\\b\"), timestamp, author, raw_text.\n• Write to DB via SQLAlchemy session; bulk upserts.\n• Schedule via schedule.every(1).minutes.\n• Expose CLI ‘python -m scraper run’",
        "testStrategy": "Unit test parser on saved HTML fixtures. Integration test hits live site with env var LIVE=true, asserts >0 posts saved. DB row count grows after run.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Scraper Base Class",
            "description": "Create the abstract Scraper class with mandatory fetch() and parse() methods and shared helpers/attributes.",
            "dependencies": [],
            "details": "File: scraper/base.py; use abc.ABCMeta; include user_agent, session, and __init__ with base_url; add basic error handling and logging stubs.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop Shared Scraper Utilities",
            "description": "Implement utilities for header randomization, polite delay (3-8 s), robots.txt compliance, and Selenium fallback wrapper.",
            "dependencies": [
              "3.1"
            ],
            "details": "Files: scraper/utils/headers.py, delay.py, robots.py, selenium_wrapper.py; expose callable functions/classes reusable by all forum scrapers.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Hegnar Forum Scraper",
            "description": "Build hegnar.py scraper subclass to fetch forum pages, parse HTML, and extract ticker, timestamp, author, and raw_text.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Use requests+BeautifulSoup; pagination support; regex r\"\\b[A-Z]{2,4}\\b\" for ticker; yield Post objects; handle anti-bot blocks with retry/backoff.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Avanza Forum Scraper with Selenium Fallback",
            "description": "Create avanza.py scraper that first tries requests+BS4 and falls back to Selenium when JS rendering required.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Detect JS-rendered content via heuristic; manage headless Chrome driver; reuse shared utilities; extract same fields as Hegnar scraper.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate Persistence Layer",
            "description": "Wire both scrapers to SQLAlchemy session for bulk upsert of posts into the database, respecting unique constraints.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "Add db/session.py factory; implement upsert_posts(posts, session); ensure forum and ticker FK resolution; commit in batches.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add Scheduling, CLI Entry-Point, and Logging",
            "description": "Expose `python -m scraper run` command that schedules both scrapers every minute with structured logging.",
            "dependencies": [
              "3.1",
              "3.3",
              "3.4",
              "3.5"
            ],
            "details": "Use schedule library; configure logging via logging.config.dictConfig; graceful shutdown on SIGINT; argparse flags for interval, verbose.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Write Unit and Integration Tests",
            "description": "Create pytest suite with HTML fixtures for parsers and live-site integration tests gated by LIVE=true env var.",
            "dependencies": [
              "3.3",
              "3.4",
              "3.5",
              "3.6"
            ],
            "details": "Mock network calls with responses; assert ticker extraction accuracy; integration test checks >0 new DB rows after run; CI workflow step.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Build NLP Sentiment Pipeline",
        "description": "Create service that cleans Norwegian/Swedish text, runs HF model, stores score.",
        "details": "• nlp/pipeline.py: load ‘NbAiLab/nb-bert-base’ & ‘KBLab/swe-bert’ via transformers.\n• Language detect simple heuristic by forum locale.\n• Preprocess: lower, strip URLs, emoji removal, finance slang lexicon mapping.\n• For each new post without sentiment_score, batch encode → model → softmax[positive].\n• Persist score back to posts table.\n• CLI ‘python -m nlp run’ scheduled every 2 min.",
        "testStrategy": "Unit test preprocessing utils. Mock model returning fixed logits and assert DB update. End-to-end: insert 10 raw posts → run pipeline → sentiment_score not NULL.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Lightweight Language Detection Heuristic",
            "description": "Create a simple, fast heuristic to determine whether a post is Norwegian or Swedish based on forum locale or character patterns.",
            "dependencies": [],
            "details": "Add nlp/lang_detect.py with a function detect_lang(text, locale_hint) → 'no' | 'sv'. Use locale metadata first, fall back to character bigram frequency thresholds. Unit-test with mixed examples.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create Text Pre-processing Utilities",
            "description": "Develop utilities to clean URLs, remove emojis, lowercase text, and map finance slang to canonical forms.",
            "dependencies": [],
            "details": "Add nlp/preprocess.py with clean_text(text) pipeline. Include regex URL stripping, emoji regex, slang dict load from JSON. Provide standalone CLI for quick testing and unit tests for each transform.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Wrap HF Model Loading with Caching",
            "description": "Implement model loader that lazily downloads and caches Norwegian and Swedish BERT sentiment heads.",
            "dependencies": [],
            "details": "Add nlp/model.py with get_model(lang) that returns (tokenizer, model) from 'NbAiLab/nb-bert-base' or 'KBLab/swe-bert'. Use transformers.AutoModelForSequenceClassification & AutoTokenizer with torch_dtype inference and local_dir cache.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Develop Batch Inference Logic",
            "description": "Create batched sentiment inference that encodes posts, runs model forward pass, and extracts softmax positive probability.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3"
            ],
            "details": "Add nlp/infer.py with run_inference(posts, lang). Use torch.no_grad(), DataLoader with batch_size configurable, device auto-select. Return list[{'post_id': id, 'score': float}]. Capture runtime metrics.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Build Database Interaction Layer",
            "description": "Implement queries to fetch posts lacking sentiment_score and persist computed scores in atomic transactions.",
            "dependencies": [
              "4.4"
            ],
            "details": "Add nlp/db_io.py using SQLAlchemy session. Function fetch_unscored(limit) and save_scores(score_rows). Include retry logic & index hint comment for performance.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Integrate Scandinavian News & Company Announcements via OpenBB",
            "description": "Extend the data ingestion layer to pull Scandinavian stock-related news and official company announcements using the OpenBB SDK and Nordic exchange APIs.",
            "dependencies": [
              2
            ],
            "details": "1. Folder structure\n   • market/news_openbb.py – wraps OpenBB news endpoints.\n   • market/announcements.py – fetches company filings from Oslo Børs NewsWeb and Nasdaq OMX Nordic APIs.\n   • schemas/news.py – SQLAlchemy model update (add columns source, importance, body_html, created_at_idx).\n   • config/markets.yml – map ISIN⇔ticker⇔exchange to drive look-ups.\n\n2. OpenBB integration\n   • Use openbb.stocks.news(ticker, start, limit=50) which returns headline, link, summary, datetime.\n   • For each Scandinavian ticker in config, call once per hour (schedule.every().hour).\n   • Convert UTC → Europe/Oslo timezone, store in news table, set source='openbb'.\n   • Implement upsert on (ticker, headline, published) to avoid duplicates.\n\n3. Official announcements\n   • Oslo Børs NewsWeb: GET https://newsweb.oslobors.no/api/v1/news?issuerId={id}&from={ISO}.\n   • Nasdaq OMX Nordic filings: RSS https://www.nasdaqomxnordic.com/shares/NewsRelease?Instrument=SE{RIC}. Parse with feedparser.\n   • Normalize fields (headline, body_html, url, published, category='filing', importance=1.0).\n\n4. Incremental state\n   • news_state.json persisted in cache/ to hold last_fetch_ts per source.\n   • If last_fetch_ts is >48h ago, back-fill via pagination.\n\n5. CLI & scheduler\n   • entry point: python -m market.news_sync run --backfill 7d\n   • Register with existing cron container (Task 9) via schedule.every(10).minutes.\n\n6. Logging & metrics\n   • Use structlog; emit gauge ``news_items_fetched`` to Prometheus pushgateway if env var PROM_PUSHGATEWAY is set.\n\n7. Security & rate limits\n   • Respect OpenBB free tier rate: max 60 calls/min. Implement asyncio.Semaphore(50) around requests.\n   • Cache identical calls inside 5-minute TTL using functools.lru_cache.\n\n8. Documentation\n   • docs/INGEST_NEWS.md covering field mapping, env vars (OPENBB_API_KEY), troubleshooting.",
            "status": "pending",
            "priority": "high",
            "testStrategy": "1. Unit tests (pytest)\n   • Mock openbb.stocks.news to return sample JSON; assert  ›10 items parsed and persisted, duplicate keys ignored.\n   • Mock Oslo Børs and Nasdaq Nordic endpoints with responses library, covering pagination and malformed entries.\n\n2. Integration tests with VCR.py\n   • Record live call for ticker 'EQNR'. Replay and verify at least one row insert.\n\n3. State recovery test\n   • Set news_state.json with last_fetch_ts 24h ago, run sync, ensure only items >24h are fetched.\n\n4. Container test\n   • `docker-compose run market news_sync --backfill 1d`; exit code 0 and stdout contains \"Saved\" count.\n\n5. Performance test\n   • Time sync for 200 tickers ≤ 90 s on GitHub CI runner; fail if slower.",
            "subtasks": []
          },
          {
            "id": 7,
            "title": "Write Unit & Integration Tests",
            "description": "Create tests covering preprocessing, language detection, model wrapper (mocked), DB I/O, and end-to-end pipeline execution.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4",
              "4.5",
              "4.6"
            ],
            "details": "Add tests/ with pytest fixtures: mock_model returning fixed logits, temporary SQLite DB seeded with 10 sample posts. Assert sentiment_score populated and CLI exit code 0. Include GitHub Actions job reference for Task 10.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Aggregate Sentiment & Detect Buzz Anomalies",
        "description": "Compute rolling sentiment per ticker and detect unusual spike in volume or polarity.",
        "details": "• analytics/aggregator.py: query last N hours posts, groupby ticker 5-min window using pandas.\n• Store results into sentiment_agg.\n• Anomaly: z-score over 24h mean of post_cnt; threshold >2.\n• anomalies table(id,ticker,window_start,zscore,direction)\n• Scheduled hourly.",
        "testStrategy": "Unit test z-score logic with synthetic data. Integration test: load fixture counts → expect anomaly flagged when spike 3x.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Recent Posts Query",
            "description": "Use SQLAlchemy to retrieve posts and their sentiment scores from the last N hours per ticker.",
            "dependencies": [],
            "details": "Create function get_recent_posts(session, hours_back) in analytics/aggregator.py. Ensure proper indexes on posts.timestamp and posts.ticker for performance.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Compute 5-Minute Window Aggregates",
            "description": "Group the queried data by ticker and 5-minute window using pandas to calculate avg_score and post_cnt.",
            "dependencies": [
              "5.1"
            ],
            "details": "Use pandas.Grouper(freq='5min') on a DataFrame built from step 1 results. Output DataFrame columns: ticker, interval_start, interval_end, avg_score, post_cnt.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Persist Aggregates to sentiment_agg",
            "description": "Insert or upsert the computed aggregates into the sentiment_agg table.",
            "dependencies": [
              "5.2"
            ],
            "details": "Leverage SQLAlchemy bulk operations with ON CONFLICT DO UPDATE for upsert logic. Wrap in transaction and commit.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Detect Anomalies and Record to anomalies Table",
            "description": "Calculate z-score over 24-hour rolling mean of post_cnt per ticker and store spikes crossing threshold ±2.",
            "dependencies": [
              "5.3"
            ],
            "details": "For each ticker, maintain 24h history using sentiment_agg. Compute z = (current_cnt-mean)/std. Insert into anomalies table with direction = 'positive' or 'negative'.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Schedule Hourly Aggregation Job & Optimize DB",
            "description": "Configure scheduler to run aggregator hourly and add necessary DB optimizations.",
            "dependencies": [
              "5.3",
              "5.4"
            ],
            "details": "Use schedule or APScheduler in a CLI entry point. Create indexes on sentiment_agg(ticker, interval_start) and anomalies(ticker, window_start). Ensure job logs metrics.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create Unit Tests for Aggregation & Anomaly Logic",
            "description": "Write pytest cases with synthetic data to validate aggregation correctness and anomaly detection thresholds.",
            "dependencies": [
              "5.4"
            ],
            "details": "Generate synthetic DataFrame, run through aggregation/anomaly functions, and assert expected rows in sentiment_agg and anomalies. Use pytest fixtures and in-memory SQLite.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate Scandinavian News & Company Announcements via OpenBB",
        "description": "Extend the data ingestion layer to pull Scandinavian stock-related news and official company announcements using the OpenBB SDK and Nordic exchange APIs.",
        "details": "1. Folder structure\n   • market/news_openbb.py – wraps OpenBB news endpoints.\n   • market/announcements.py – fetches company filings from Oslo Børs NewsWeb and Nasdaq OMX Nordic APIs.\n   • schemas/news.py – SQLAlchemy model update (add columns source, importance, body_html, created_at_idx).\n   • config/markets.yml – map ISIN⇔ticker⇔exchange to drive look-ups.\n\n2. OpenBB integration\n   • Use openbb.stocks.news(ticker, start, limit=50) which returns headline, link, summary, datetime.\n   • For each Scandinavian ticker in config, call once per hour (schedule.every().hour).\n   • Convert UTC → Europe/Oslo timezone, store in news table, set source='openbb'.\n   • Implement upsert on (ticker, headline, published) to avoid duplicates.\n\n3. Official announcements\n   • Oslo Børs NewsWeb: GET https://newsweb.oslobors.no/api/v1/news?issuerId={id}&from={ISO}.\n   • Nasdaq OMX Nordic filings: RSS https://www.nasdaqomxnordic.com/shares/NewsRelease?Instrument=SE{RIC}. Parse with feedparser.\n   • Normalize fields (headline, body_html, url, published, category='filing', importance=1.0).\n\n4. Incremental state\n   • news_state.json persisted in cache/ to hold last_fetch_ts per source.\n   • If last_fetch_ts is >48h ago, back-fill via pagination.\n\n5. CLI & scheduler\n   • entry point: python -m market.news_sync run --backfill 7d\n   • Register with existing cron container (Task 9) via schedule.every(10).minutes.\n\n6. Logging & metrics\n   • Use structlog; emit gauge ``news_items_fetched`` to Prometheus pushgateway if env var PROM_PUSHGATEWAY is set.\n\n7. Security & rate limits\n   • Respect OpenBB free tier rate: max 60 calls/min. Implement asyncio.Semaphore(50) around requests.\n   • Cache identical calls inside 5-minute TTL using functools.lru_cache.\n\n8. Documentation\n   • docs/INGEST_NEWS.md covering field mapping, env vars (OPENBB_API_KEY), troubleshooting.",
        "testStrategy": "1. Unit tests (pytest)\n   • Mock openbb.stocks.news to return sample JSON; assert  ›10 items parsed and persisted, duplicate keys ignored.\n   • Mock Oslo Børs and Nasdaq Nordic endpoints with responses library, covering pagination and malformed entries.\n\n2. Integration tests with VCR.py\n   • Record live call for ticker 'EQNR'. Replay and verify at least one row insert.\n\n3. State recovery test\n   • Set news_state.json with last_fetch_ts 24h ago, run sync, ensure only items >24h are fetched.\n\n4. Container test\n   • `docker-compose run market news_sync --backfill 1d`; exit code 0 and stdout contains \"Saved\" count.\n\n5. Performance test\n   • Time sync for 200 tickers ≤ 90 s on GitHub CI runner; fail if slower.",
        "status": "done",
        "priority": "high",
        "dependencies": [
          2
        ],
        "subtasks": [
          {
            "id": 1,
            "title": "Update Data Models and Configuration for News & Announcements",
            "description": "Extend the SQLAlchemy news schema and update configuration files to support new fields and Scandinavian ticker mappings.",
            "dependencies": [],
            "details": "Modify schemas/news.py to add columns: source, importance, body_html, and created_at_idx. Update config/markets.yml to map ISIN, ticker, and exchange for Scandinavian equities. Ensure the schema supports both news and official announcements, and that configuration enables lookup for all relevant tickers and exchanges.",
            "status": "done",
            "testStrategy": "Run alembic migrations and verify schema changes. Validate config lookups for a sample of Scandinavian tickers."
          },
          {
            "id": 2,
            "title": "Implement OpenBB News Ingestion Wrapper",
            "description": "Develop market/news_openbb.py to fetch and normalize news for Scandinavian tickers using the OpenBB SDK.",
            "dependencies": [
              "6.1"
            ],
            "details": "Use openbb.stocks.news(ticker, start, limit=50) to fetch news for each ticker in config. Normalize fields (headline, link, summary, datetime) and convert UTC to Europe/Oslo timezone. Implement upsert logic on (ticker, headline, published) to avoid duplicates. Store results in the news table with source='openbb'.",
            "status": "done",
            "testStrategy": "Mock OpenBB SDK responses and assert correct parsing, normalization, and upsert behavior. Test hourly scheduling logic."
          },
          {
            "id": 3,
            "title": "Integrate Official Company Announcements from Nordic Exchanges",
            "description": "Create market/announcements.py to fetch, parse, and normalize company filings from Oslo Børs NewsWeb and Nasdaq OMX Nordic APIs.",
            "dependencies": [
              "6.1"
            ],
            "details": "Implement GET requests to Oslo Børs NewsWeb and parse Nasdaq OMX Nordic RSS feeds using feedparser. Normalize fields (headline, body_html, url, published, category='filing', importance=1.0). Store announcements in the news table, ensuring deduplication and correct field mapping.",
            "status": "done",
            "testStrategy": "Mock API and RSS responses, including pagination and malformed entries. Assert correct normalization and persistence."
          },
          {
            "id": 4,
            "title": "Implement Incremental State Management and Backfill Logic",
            "description": "Add logic to persist and manage news_state.json for tracking last fetch timestamps and support backfilling.",
            "dependencies": [
              "6.2",
              "6.3"
            ],
            "details": "Persist last_fetch_ts per source in cache/news_state.json. On each run, check if last_fetch_ts is >48h ago and trigger backfill with pagination. Ensure state is updated after each successful fetch and that backfill covers missed intervals.",
            "status": "done",
            "testStrategy": "Simulate missed fetches and verify backfill logic retrieves all missing news. Assert state file updates correctly."
          },
          {
            "id": 5,
            "title": "Develop CLI Entry Point, Scheduler, Logging, and Metrics",
            "description": "Create a CLI entry point for news sync, integrate with the scheduler, add logging, metrics, and enforce rate limits and caching.",
            "dependencies": [
              "6.4"
            ],
            "details": "Implement python -m market.news_sync run --backfill 7d as the entry point. Register with the cron container to run every 10 minutes. Use structlog for logging and emit news_items_fetched to Prometheus if PROM_PUSHGATEWAY is set. Enforce OpenBB rate limits with asyncio.Semaphore(50) and cache identical calls with functools.lru_cache (5-min TTL). Document field mapping, environment variables, and troubleshooting in docs/INGEST_NEWS.md.",
            "status": "done",
            "testStrategy": "Run end-to-end integration tests with the CLI, verify scheduling, logging, metrics emission, rate limiting, and caching. Review generated documentation for completeness."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Market Price Data Integration via OpenBB",
        "description": "Integrate OpenBB SDK to pull price data aligned by ticker+timestamp.",
        "details": "• market/data.py: openbb.stocks.price(ticker, interval=1) hourly.\n• Store market_prices(id,ticker,price,ts).\n• Schedule via schedule library.",
        "testStrategy": "Integration test with sandbox ticker 'EQNR'. Assert price row exists. Use VCR.py to record HTTP.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design SQLAlchemy Models & Migrations",
            "description": "Create market_prices table with appropriate columns, indices, and Alembic migration scripts.",
            "dependencies": [],
            "details": "• Define models in db/models.py with id UUID PK, ticker FK, timestamps.\n• Add unique constraints on (ticker, ts) for market_prices.\n• Generate Alembic revision, test upgrade/downgrade on local Postgres.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate OpenBB SDK for Price Retrieval",
            "description": "Fetch hourly prices via openbb.stocks.price with retry/backoff and rate-limiting middleware.",
            "dependencies": [
              "7.1"
            ],
            "details": "• Wrapper in market/data.py with tenacity retry, backoff ≤60s, and async semaphore rate limiter.\n• Normalize response into model schema and commit via session.\n• Handle API errors and log.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Schedule Periodic Fetch Jobs with Deduplication",
            "description": "Set up schedule library to run price fetchers at defined intervals with idempotency.",
            "dependencies": [
              "7.2"
            ],
            "details": "• Hourly job for prices.\n• Maintain last-checked timestamps; skip if already processed.\n• Add CLI entrypoint scheduler/main.py.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Write Integration Tests with VCR.py",
            "description": "Ensure end-to-end data pipeline stores rows and avoids duplicates using recorded HTTP interactions.",
            "dependencies": [
              "7.3"
            ],
            "details": "• Use pytest + VCR.py cassettes for OpenBB requests.\n• Seed in-memory Postgres, run scheduler, assert expected row counts.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Develop Streamlit Dashboard",
        "description": "Interactive dashboard showing Top Buzzing Stocks, sentiment charts, news overlay.",
        "details": "• dashboard/app.py using streamlit.\n• Sidebar date range & ticker filter.\n• Main: Plotly heatmap of post_cnt_zscore, line chart sentiment vs price.\n• Use SQLAlchemy queries, cache via st.cache_data.\n• Dockerfile exposes port 8501.",
        "testStrategy": "Run `streamlit run app.py` inside container; Cypress e2e tests open dashboard, assert heatmap renders and API queries <1s.",
        "priority": "medium",
        "dependencies": [
          5,
          6,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Streamlit app skeleton and configuration",
            "description": "Create the initial Streamlit application structure with Docker support and basic run script.",
            "dependencies": [],
            "details": "• Create dashboard/app.py with placeholder layout (sidebar + main).\n• Configure Streamlit in .streamlit/config.toml (theme, page title, wide layout).\n• Update Dockerfile to install streamlit and expose port 8501; add CMD \"streamlit run dashboard/app.py --server.port 8501 --server.address 0.0.0.0\".\n• Verify local run: `streamlit run dashboard/app.py` renders empty page.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement cached data layer functions",
            "description": "Write SQLAlchemy-based functions (wrapped in st.cache_data) to fetch sentiments, prices, and news in required formats.",
            "dependencies": [
              "8.1"
            ],
            "details": "• Connect via SQLAlchemy engine from config.\n• Functions: get_buzzing_heatmap_data(start,end), get_sentiment_price_series(ticker,start,end), get_news_overlay(ticker,start,end).\n• Apply @st.cache_data(ttl=300).\n• Return pandas DataFrames ready for Plotly.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build Plotly heatmap for Top Buzzing Stocks",
            "description": "Render heatmap of post_cnt_zscore by ticker over selected period using Plotly in main page.",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "• Use get_buzzing_heatmap_data() output.\n• Create Plotly heatmap figure with color-scale diverging centered at 0.\n• Add hover showing post count, z-score.\n• Integrate figure into Streamlit container.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build sentiment vs price line chart with news overlay",
            "description": "Combine dual-axis line chart of sentiment average and price, overlaying news event markers.",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "• get_sentiment_price_series() returns sentiment and price columns.\n• Plotly secondary y-axis: left = sentiment, right = price.\n• get_news_overlay() provides vertical lines + tooltip headlines.\n• Add range slider & adaptive x-axis ticks.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add sidebar filters and state management",
            "description": "Implement date range picker, ticker multiselect, and internal session state to synchronize charts.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3",
              "8.4"
            ],
            "details": "• st.sidebar.date_input for start/end, st.sidebar.multiselect for tickers.\n• Use st.session_state to propagate selections.\n• Trigger data reload functions with selected parameters; ensure minimal reruns.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Write Cypress end-to-end tests and verify container exposure",
            "description": "Automate E2E testing of dashboard rendering and performance, and confirm Docker port mapping.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3",
              "8.4",
              "8.5"
            ],
            "details": "• Add cypress/ directory with tests: visit '/', assert heatmap and line chart load within 1 s.\n• Use cy.request to validate backend API latency.\n• docker-compose override maps 8501:8501; GitHub Action runs `docker-compose up -d dashboard && npx cypress run`.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 9,
        "title": "Containerize Cron & Worker Services",
        "description": "Docker-compose orchestration of scraper, nlp, aggregator, alert, dashboard services.",
        "details": "• Extend docker-compose.yml: services: db, scraper, nlp, aggregator, alert, market, dashboard.\n• Use healthchecks. Networks internal.\n• environment variables via .env file.",
        "testStrategy": "`docker-compose up -d` then `docker-compose ps` shows all healthy. Logs show scheduled jobs executing. curl localhost:8501 returns HTTP 200.",
        "priority": "low",
        "dependencies": [
          3,
          4,
          5,
          6,
          7,
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Service Dockerfiles / Base Image",
            "description": "Author individual Dockerfiles or a shared base image for scraper, nlp, aggregator, alert, market, dashboard services.",
            "dependencies": [],
            "details": "• Decide between shared Python base image vs per-service Dockerfile.\n• Install OS libs (curl, git, cron) and Python 3.11 + poetry/requirements.\n• Copy service code, install dependencies, set non-root user.\n• Output images tagged nssm/<service>:latest for use in compose.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Extend docker-compose.yml with Services & Networks",
            "description": "Add all worker services plus db to docker-compose and define internal networks.",
            "dependencies": [
              "9.1"
            ],
            "details": "• Define services: db, scraper, nlp, aggregator, alert, market, dashboard.\n• Reference images built in 9.1 or build context paths.\n• Create internal network \"nssm_net\"; attach all containers.\n• Mount volumes for persistent db data and logs.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Healthchecks & Environment Variables",
            "description": "Introduce per-service healthcheck commands and centralised .env configuration.",
            "dependencies": [
              "9.2"
            ],
            "details": "• Add healthcheck sections (curl /health, pg_isready, etc.) with suitable intervals/timeouts.\n• Create .env file holding DB creds, API keys, scheduler intervals.\n• Map environment: key=value or env_file in compose.\n• Fail containers on unhealthy status for easier orchestration.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Entrypoint Scripts & Cron Scheduling",
            "description": "Embed cron or schedule triggers inside containers to run workers at desired intervals.",
            "dependencies": [
              "9.2",
              "9.3"
            ],
            "details": "• Write entrypoint.sh per service: migrate DB, start cron or python scheduler.\n• Use crontab files or schedule lib; e.g., scraper every minute, aggregator hourly.\n• Ensure logs stream to stdout for docker-compose logging.\n• Update Dockerfiles (9.1) to COPY entrypoint & set CMD.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Local Orchestration Verification & Documentation",
            "description": "Test full stack with docker-compose and write README usage instructions.",
            "dependencies": [
              "9.3",
              "9.4"
            ],
            "details": "• Run `docker-compose up -d`; verify `docker-compose ps` shows healthy.\n• Tail logs to confirm cron jobs trigger and DB writes occur.\n• Hit dashboard on localhost:8501, expect HTTP 200.\n• Document setup, env vars, common commands, troubleshooting.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "Automated Testing & CI Pipeline",
        "description": "Configure GitHub Actions pipeline for linting, tests, build images.",
        "details": "• .github/workflows/ci.yml: matrix python 3.10/3.11.\n• Steps: checkout, setup-python, cache pip, install, run flake8 & black --check, pytest, build docker-compose build.\n• Push images to GHCR for main branch.",
        "testStrategy": "Push PR with failing lint → action fails. Fix → action passes and images appear in registry.",
        "priority": "low",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Base GitHub Actions Workflow",
            "description": "Set up a ci.yml workflow with Python version matrix and pip caching.",
            "dependencies": [],
            "details": "• Path: .github/workflows/ci.yml\n• Trigger: pull_request + push\n• Strategy.matrix: python-version [3.10, 3.11]\n• Steps: actions/checkout, actions/setup-python (uses matrix), actions/cache (key: v1-${{ runner.os }}-${{ matrix.python-version }}-pip), pip install -r requirements.txt\n• Verify workflow syntax via act locally.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate Linting Checks",
            "description": "Add black, isort, and flake8 steps that fail on style violations.",
            "dependencies": [
              "10.1"
            ],
            "details": "• Extend ci.yml with pip install 'black==23.*' 'isort==5.*' 'flake8==6.*'\n• Run commands: black --check . ; isort --check-only . ; flake8 .\n• Annotate failures using github-sane-report or default annotations.\n• Ensure non-zero exit stops job.\n• Update README with badge for build status.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Run Pytest with Postgres Service",
            "description": "Configure workflow to spin up Postgres container and execute pytest suite.",
            "dependencies": [
              "10.2"
            ],
            "details": "• Add services.postgres in ci.yml (image: postgres:15, env: POSTGRES_PASSWORD, ports)\n• Wait for DB readiness using health-check or bash retry loop.\n• Install test deps via pip install -r requirements-test.txt\n• Command: pytest -q --maxfail=1 --disable-warnings\n• Artifacts: upload coverage.xml and junit.xml.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build & Push Docker Images to GHCR",
            "description": "On main branch, build multi-arch Docker image and push to GitHub Container Registry.",
            "dependencies": [
              "10.3"
            ],
            "details": "• Add job conditioned on if: github.ref == 'refs/heads/main'\n• Steps: docker/setup-buildx-action, docker/login-action (registry: ghcr.io, using GITHUB_TOKEN), docker/metadata-action for tags (sha, latest), docker/build-push-action with push: true\n• Optionally cache layers.\n• Verify image appears under ghcr.io/org/repo.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-26T13:47:28.377Z",
      "updated": "2025-09-01T09:03:51.387Z",
      "description": "Tasks for master context"
    }
  }
}