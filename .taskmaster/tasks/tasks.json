{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Python Project & Repository",
        "description": "Create git repo, Python virtual environment, and baseline directory structure for NSSM modules (scraper, db, nlp, dashboard).",
        "details": "• mkdir NSSM/{scraper,db,nlp,dashboard,config,tests}\n• git init; create .gitignore (venv, __pycache__, .env)\n• poetry init or pip-tools requirements.txt with python 3.11\n• Add core deps: requests, beautifulsoup4, psycopg2-binary, sqlalchemy, pandas, scikit-learn, transformers, streamlit, plotly, schedule, python-dotenv\n• Setup pre-commit hooks: black, isort, flake8\n• Dockerfile: slim-buster, copy src, install deps\n• docker-compose.yml with app + postgres 15.\n• Seed README explaining architecture.",
        "testStrategy": "Run ‘docker-compose up’ → container builds without errors. Execute ‘pytest -q’ (empty suite) returns 0. Verify pre-commit runs on git commit.",
        "priority": "high",
        "dependencies": [],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Git Repository & Seed README",
            "description": "Create project root, initialize git repository, and add a baseline README describing NSSM architecture.",
            "dependencies": [],
            "details": "- Create project directory `NSSM` and navigate into it.\n- Run `git init` to initialize repository.\n- Add initial `README.md` including project overview, module descriptions, and contribution guidelines.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Generate Python Packaging & Dependency Management",
            "description": "Set up Poetry with Python 3.11 and define core dependencies for the project.",
            "dependencies": [
              "1.1"
            ],
            "details": "- Run `poetry init --name NSSM --python ^3.11`.\n- Add dependencies: requests, beautifulsoup4, psycopg2-binary, sqlalchemy, pandas, scikit-learn, transformers, streamlit, plotly, schedule, python-dotenv.\n- Configure Poetry virtual environment and ensure `poetry.lock` is committed.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Establish Directory Structure with Module Stubs",
            "description": "Create baseline folder hierarchy and `__init__.py` files for scraper, db, nlp, dashboard, config, and tests.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "- Execute `mkdir -p NSSM/{scraper,db,nlp,dashboard,config,tests}`.\n- Add `__init__.py` to each package directory.\n- Include placeholder test file `tests/test_placeholder.py`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Configure Development Tooling",
            "description": "Add .gitignore, pre-commit hooks (black, isort, flake8), and pytest boilerplate for consistent development workflow.",
            "dependencies": [
              "1.1",
              "1.2",
              "1.3"
            ],
            "details": "- Create `.gitignore` covering venv, `__pycache__`, `.env`, and build artifacts.\n- Write `.flake8`, `pyproject.toml` or separate config for black & isort.\n- Install and configure `pre-commit`; add hooks for black, isort, flake8.\n- Add minimal `pytest.ini` and verify `pytest -q` returns 0.",
            "status": "in-progress",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add Containerization Assets & Verify Compose",
            "description": "Create Dockerfile and docker-compose.yml with Postgres service; ensure `docker-compose up` builds and runs successfully.",
            "dependencies": [
              "1.2",
              "1.3",
              "1.4"
            ],
            "details": "- Write `Dockerfile` based on python:3.11-slim-buster, copy source, and install dependencies via Poetry.\n- Create `docker-compose.yml` defining app service and Postgres 15 database.\n- Run `docker-compose up --build` and verify containers start without errors.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Design & Migrate PostgreSQL Schema",
        "description": "Define relational schema for posts, tickers, sentiment, alerts and create migration script.",
        "details": "• Use SQLAlchemy declarative models in db/models.py\n• Tables: forums(id, name,url), posts(id, forum_id, ticker, timestamp, author, raw_text, clean_text, sentiment_score FLOAT), sentiment_agg(id,ticker,interval_start,interval_end,avg_score,post_cnt,created_at), alerts(id,ticker,rule,triggered_at)\n• Alembic revision to create tables.\n• Use TimescaleDB extension for sentiment_agg hypertable (optional enable if docker-compose env TS).",
        "testStrategy": "Run ‘alembic upgrade head’ in container. Connect via psql → \\dt lists all tables. Insert mock row → SELECT verifies constraints. pytest with in-memory postgres (pytest-postgresql) validating model round-trip.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up SQLAlchemy Base and Engine",
            "description": "Create the foundational database module with Base class and engine configuration.",
            "dependencies": [],
            "details": "Add db/__init__.py; define Base = declarative_base(); read DATABASE_URL from env; expose get_engine() and get_session() helpers; include basic logging.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Declarative Models",
            "description": "Define ORM classes for forums, posts, sentiment_agg, alerts, anomalies, and market tables.",
            "dependencies": [
              "2.1"
            ],
            "details": "Create db/models.py; add columns, primary/foreign keys, indexes, relationships, __repr__; enforce NOT NULL & unique constraints where appropriate.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Alembic and Generate Initial Migration",
            "description": "Initialize Alembic, link metadata, and create first revision to build core tables.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "Run `alembic init migrations`; update env.py with target_metadata = Base.metadata; generate revision 'create_core_tables'; test `alembic upgrade head` against local Postgres.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add TimescaleDB Extension & Hypertable Logic",
            "description": "Extend migration script to optionally enable TimescaleDB and convert sentiment_agg to hypertable.",
            "dependencies": [
              "2.3"
            ],
            "details": "Within upgrade(): check os.getenv('TS'); execute `CREATE EXTENSION IF NOT EXISTS timescaledb`; run `SELECT create_hypertable('sentiment_agg', 'interval_start', if_not_exists=>TRUE)`; include corresponding downgrade steps.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write Seeds, Fixtures, and Unit Tests",
            "description": "Create seed scripts and pytest fixtures to validate CRUD operations with the new schema.",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "Add db/seed.py populating mock forums and posts; configure pytest-postgresql fixture applying migrations; assert insert/select/update/delete for each model; include tox entry.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Update Docker Compose and Automate Migration",
            "description": "Modify docker-compose to include env vars, volumes, and run Alembic upgrade during container start.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "Add TIMESCALEDB flag, mount ./migrations; in app service CMD run `alembic upgrade head`; verify by `docker-compose up` then `psql -c '\\dt'`; document process in README.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Forum Scraper (Hegnar & Avanza)",
        "description": "Build resilient, polite scrapers that run continuously and store raw posts.",
        "details": "• scraper/base.py defines Scraper abstract class(fetch(),parse())\n• Implement hegnar.py, avanza.py using requests+bs4; fallback to Selenium for JS.\n• Respect robots.txt but randomize headers, add 3-8s sleep.\n• Extract (ticker regex like r\"\\b[A-Z]{2,4}\\b\"), timestamp, author, raw_text.\n• Write to DB via SQLAlchemy session; bulk upserts.\n• Schedule via schedule.every(1).minutes.\n• Expose CLI ‘python -m scraper run’",
        "testStrategy": "Unit test parser on saved HTML fixtures. Integration test hits live site with env var LIVE=true, asserts >0 posts saved. DB row count grows after run.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Scraper Base Class",
            "description": "Create the abstract Scraper class with mandatory fetch() and parse() methods and shared helpers/attributes.",
            "dependencies": [],
            "details": "File: scraper/base.py; use abc.ABCMeta; include user_agent, session, and __init__ with base_url; add basic error handling and logging stubs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop Shared Scraper Utilities",
            "description": "Implement utilities for header randomization, polite delay (3-8 s), robots.txt compliance, and Selenium fallback wrapper.",
            "dependencies": [
              "3.1"
            ],
            "details": "Files: scraper/utils/headers.py, delay.py, robots.py, selenium_wrapper.py; expose callable functions/classes reusable by all forum scrapers.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Hegnar Forum Scraper",
            "description": "Build hegnar.py scraper subclass to fetch forum pages, parse HTML, and extract ticker, timestamp, author, and raw_text.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Use requests+BeautifulSoup; pagination support; regex r\"\\b[A-Z]{2,4}\\b\" for ticker; yield Post objects; handle anti-bot blocks with retry/backoff.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Avanza Forum Scraper with Selenium Fallback",
            "description": "Create avanza.py scraper that first tries requests+BS4 and falls back to Selenium when JS rendering required.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Detect JS-rendered content via heuristic; manage headless Chrome driver; reuse shared utilities; extract same fields as Hegnar scraper.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate Persistence Layer",
            "description": "Wire both scrapers to SQLAlchemy session for bulk upsert of posts into the database, respecting unique constraints.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "Add db/session.py factory; implement upsert_posts(posts, session); ensure forum and ticker FK resolution; commit in batches.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add Scheduling, CLI Entry-Point, and Logging",
            "description": "Expose `python -m scraper run` command that schedules both scrapers every minute with structured logging.",
            "dependencies": [
              "3.1",
              "3.3",
              "3.4",
              "3.5"
            ],
            "details": "Use schedule library; configure logging via logging.config.dictConfig; graceful shutdown on SIGINT; argparse flags for interval, verbose.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Write Unit and Integration Tests",
            "description": "Create pytest suite with HTML fixtures for parsers and live-site integration tests gated by LIVE=true env var.",
            "dependencies": [
              "3.3",
              "3.4",
              "3.5",
              "3.6"
            ],
            "details": "Mock network calls with responses; assert ticker extraction accuracy; integration test checks >0 new DB rows after run; CI workflow step.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Build NLP Sentiment Pipeline",
        "description": "Create service that cleans Norwegian/Swedish text, runs HF model, stores score.",
        "details": "• nlp/pipeline.py: load ‘NbAiLab/nb-bert-base’ & ‘KBLab/swe-bert’ via transformers.\n• Language detect simple heuristic by forum locale.\n• Preprocess: lower, strip URLs, emoji removal, finance slang lexicon mapping.\n• For each new post without sentiment_score, batch encode → model → softmax[positive].\n• Persist score back to posts table.\n• CLI ‘python -m nlp run’ scheduled every 2 min.",
        "testStrategy": "Unit test preprocessing utils. Mock model returning fixed logits and assert DB update. End-to-end: insert 10 raw posts → run pipeline → sentiment_score not NULL.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Lightweight Language Detection Heuristic",
            "description": "Create a simple, fast heuristic to determine whether a post is Norwegian or Swedish based on forum locale or character patterns.",
            "dependencies": [],
            "details": "Add nlp/lang_detect.py with a function detect_lang(text, locale_hint) → 'no' | 'sv'. Use locale metadata first, fall back to character bigram frequency thresholds. Unit-test with mixed examples.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create Text Pre-processing Utilities",
            "description": "Develop utilities to clean URLs, remove emojis, lowercase text, and map finance slang to canonical forms.",
            "dependencies": [],
            "details": "Add nlp/preprocess.py with clean_text(text) pipeline. Include regex URL stripping, emoji regex, slang dict load from JSON. Provide standalone CLI for quick testing and unit tests for each transform.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Wrap HF Model Loading with Caching",
            "description": "Implement model loader that lazily downloads and caches Norwegian and Swedish BERT sentiment heads.",
            "dependencies": [],
            "details": "Add nlp/model.py with get_model(lang) that returns (tokenizer, model) from 'NbAiLab/nb-bert-base' or 'KBLab/swe-bert'. Use transformers.AutoModelForSequenceClassification & AutoTokenizer with torch_dtype inference and local_dir cache.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Develop Batch Inference Logic",
            "description": "Create batched sentiment inference that encodes posts, runs model forward pass, and extracts softmax positive probability.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3"
            ],
            "details": "Add nlp/infer.py with run_inference(posts, lang). Use torch.no_grad(), DataLoader with batch_size configurable, device auto-select. Return list[{'post_id': id, 'score': float}]. Capture runtime metrics.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Build Database Interaction Layer",
            "description": "Implement queries to fetch posts lacking sentiment_score and persist computed scores in atomic transactions.",
            "dependencies": [
              "4.4"
            ],
            "details": "Add nlp/db_io.py using SQLAlchemy session. Function fetch_unscored(limit) and save_scores(score_rows). Include retry logic & index hint comment for performance.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Expose CLI & Scheduler Integration with Logging",
            "description": "Wire pipeline into a CLI entry-point and schedule it every two minutes with proper logging and Prometheus metrics.",
            "dependencies": [
              "4.4",
              "4.5"
            ],
            "details": "Create nlp/__main__.py to invoke run_pipeline(). Register console_script in pyproject.toml. Use schedule.every(2).minutes.do(run_pipeline). Hook python-json-logger and prometheus_client counters.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Write Unit & Integration Tests",
            "description": "Create tests covering preprocessing, language detection, model wrapper (mocked), DB I/O, and end-to-end pipeline execution.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4",
              "4.5",
              "4.6"
            ],
            "details": "Add tests/ with pytest fixtures: mock_model returning fixed logits, temporary SQLite DB seeded with 10 sample posts. Assert sentiment_score populated and CLI exit code 0. Include GitHub Actions job reference for Task 10.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Aggregate Sentiment & Detect Buzz Anomalies",
        "description": "Compute rolling sentiment per ticker and detect unusual spike in volume or polarity.",
        "details": "• analytics/aggregator.py: query last N hours posts, groupby ticker 5-min window using pandas.\n• Store results into sentiment_agg.\n• Anomaly: z-score over 24h mean of post_cnt; threshold >2.\n• anomalies table(id,ticker,window_start,zscore,direction)\n• Scheduled hourly.",
        "testStrategy": "Unit test z-score logic with synthetic data. Integration test: load fixture counts → expect anomaly flagged when spike 3x.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Recent Posts Query",
            "description": "Use SQLAlchemy to retrieve posts and their sentiment scores from the last N hours per ticker.",
            "dependencies": [],
            "details": "Create function get_recent_posts(session, hours_back) in analytics/aggregator.py. Ensure proper indexes on posts.timestamp and posts.ticker for performance.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Compute 5-Minute Window Aggregates",
            "description": "Group the queried data by ticker and 5-minute window using pandas to calculate avg_score and post_cnt.",
            "dependencies": [
              "5.1"
            ],
            "details": "Use pandas.Grouper(freq='5min') on a DataFrame built from step 1 results. Output DataFrame columns: ticker, interval_start, interval_end, avg_score, post_cnt.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Persist Aggregates to sentiment_agg",
            "description": "Insert or upsert the computed aggregates into the sentiment_agg table.",
            "dependencies": [
              "5.2"
            ],
            "details": "Leverage SQLAlchemy bulk operations with ON CONFLICT DO UPDATE for upsert logic. Wrap in transaction and commit.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Detect Anomalies and Record to anomalies Table",
            "description": "Calculate z-score over 24-hour rolling mean of post_cnt per ticker and store spikes crossing threshold ±2.",
            "dependencies": [
              "5.3"
            ],
            "details": "For each ticker, maintain 24h history using sentiment_agg. Compute z = (current_cnt-mean)/std. Insert into anomalies table with direction = 'positive' or 'negative'.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Schedule Hourly Aggregation Job & Optimize DB",
            "description": "Configure scheduler to run aggregator hourly and add necessary DB optimizations.",
            "dependencies": [
              "5.3",
              "5.4"
            ],
            "details": "Use schedule or APScheduler in a CLI entry point. Create indexes on sentiment_agg(ticker, interval_start) and anomalies(ticker, window_start). Ensure job logs metrics.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create Unit Tests for Aggregation & Anomaly Logic",
            "description": "Write pytest cases with synthetic data to validate aggregation correctness and anomaly detection thresholds.",
            "dependencies": [
              "5.4"
            ],
            "details": "Generate synthetic DataFrame, run through aggregation/anomaly functions, and assert expected rows in sentiment_agg and anomalies. Use pytest fixtures and in-memory SQLite.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Alerting Engine",
        "description": "Generate local notifications/logs when anomaly or sentiment flip occurs.",
        "details": "• alerts/engine.py: listen to anomalies table inserts via polling.\n• Rules configurable YAML (config/alerts.yml) e.g., post_cnt_z>2 & sentiment_avg>0.3.\n• Notify via: print to stdout, desktop notify (plyer), optional webhook POST.\n• Persist alert to alerts table.",
        "testStrategy": "Mock anomaly event, run engine → capture stdout and DB row count++. Validate webhook sent via httpbin stub.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design alert rule schema & YAML parser",
            "description": "Define a flexible rule schema and implement a parser that loads config/alerts.yml into in-memory objects.",
            "dependencies": [],
            "details": "• Specify allowed operands (>, <, >=, <=, =, !=) and logical operators (AND, OR).\n• Support metric placeholders post_cnt_z, sentiment_avg, etc.\n• Use pydantic models for validation; raise on bad config.\n• Expose get_rules() that returns list[Rule].",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Poll anomalies table & evaluate rules",
            "description": "Continuously read new rows from anomalies table and apply loaded rules to determine trigger events.",
            "dependencies": [
              "6.1"
            ],
            "details": "• Implement polling loop with last_seen_id checkpoint.\n• Map anomaly row fields to rule metrics.\n• For each anomaly evaluate all rules; yield AlertEvent objects on match.\n• Configurable poll interval via env/CLI.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement notification channels interface",
            "description": "Create notifier abstraction plus concrete stdout, desktop (plyer), and webhook POST implementations.",
            "dependencies": [
              "6.1"
            ],
            "details": "• Base class Notifier.send(AlertEvent).\n• StdoutNotifier prints JSON.\n• DesktopNotifier uses plyer.notification.\n• WebhookNotifier reads URL from env; async requests.post.\n• Pluggable via alerts.yml channel list.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Persist alerts to database with idempotency",
            "description": "Insert triggered alerts into alerts table, avoiding duplicates on retried polling cycles.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.3"
            ],
            "details": "• Define SQLAlchemy Alert model if not existing.\n• Unique constraint on (anomaly_id, rule).\n• Upsert logic (ON CONFLICT DO NOTHING).\n• Wrap in transactional session; commit per batch.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write unit tests for alerting engine",
            "description": "Mock anomalies, rules, and notifiers to verify notifications sent and DB rows created as expected.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.3",
              "6.4"
            ],
            "details": "• Use pytest + pytest-mock.\n• Fixture for in-memory SQLite DB.\n• Patch plyer and requests to capture calls.\n• Assert stdout contents.\n• Coverage target ≥90%.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "Fetch Market Data & News Integrations",
        "description": "Integrate OpenBB SDK & RSS feeds to pull price/news aligned by ticker+timestamp.",
        "details": "• market/data.py: openbb.stocks.price(ticker, interval=1) hourly.\n• news/rss.py: feedparser on Oslo Børs & Nasdaq OMX.\n• Store market_prices(id,ticker,price,ts) and news(id,ticker,title,url,published).\n• Schedule via schedule library.",
        "testStrategy": "Integration test with sandbox ticker ‘EQNR’. Assert price row exists, RSS items saved. Use VCR.py to record HTTP.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design SQLAlchemy Models & Migrations",
            "description": "Create market_prices and news tables with appropriate columns, indices, and Alembic migration scripts.",
            "dependencies": [],
            "details": "• Define models in db/models.py with id UUID PK, ticker FK, timestamps.\n• Add unique constraints on (ticker, ts) for market_prices and (ticker, url) for news.\n• Generate Alembic revision, test upgrade/downgrade on local Postgres.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate OpenBB SDK for Price Retrieval",
            "description": "Fetch hourly prices via openbb.stocks.price with retry/backoff and rate-limiting middleware.",
            "dependencies": [
              "7.1"
            ],
            "details": "• Wrapper in market/data.py with tenacity retry, backoff ≤60s, and async semaphore rate limiter.\n• Normalize response into model schema and commit via session.\n• Handle API errors and log.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement RSS Feed Ingestion & Ticker Mapping",
            "description": "Parse Oslo Børs & Nasdaq OMX feeds, map articles to tickers, and persist deduped news rows.",
            "dependencies": [
              "7.1"
            ],
            "details": "• Use feedparser to pull XML, extract title, link, published.\n• Regex / symbol map file to detect tickers in title/summary.\n• Upsert into news table, skipping existing (ticker,url).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Schedule Periodic Fetch Jobs with Deduplication",
            "description": "Set up schedule library to run price and news fetchers at defined intervals with idempotency.",
            "dependencies": [
              "7.2",
              "7.3"
            ],
            "details": "• Hourly job for prices, 15-min job for news.\n• Maintain last-checked timestamps; skip if already processed.\n• Add CLI entrypoint scheduler/main.py.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write Integration Tests with VCR.py & Sample Feeds",
            "description": "Ensure end-to-end data pipeline stores rows and avoids duplicates using recorded HTTP interactions.",
            "dependencies": [
              "7.4"
            ],
            "details": "• Use pytest + VCR.py cassettes for OpenBB & RSS requests.\n• Seed in-memory Postgres, run scheduler, assert expected row counts.\n• Include fixture CSV of sample RSS for offline test mode.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Develop Streamlit Dashboard",
        "description": "Interactive dashboard showing Top Buzzing Stocks, sentiment charts, news overlay.",
        "details": "• dashboard/app.py using streamlit.\n• Sidebar date range & ticker filter.\n• Main: Plotly heatmap of post_cnt_zscore, line chart sentiment vs price.\n• Use SQLAlchemy queries, cache via st.cache_data.\n• Dockerfile exposes port 8501.",
        "testStrategy": "Run `streamlit run app.py` inside container; Cypress e2e tests open dashboard, assert heatmap renders and API queries <1s.",
        "priority": "medium",
        "dependencies": [
          5,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Streamlit app skeleton and configuration",
            "description": "Create the initial Streamlit application structure with Docker support and basic run script.",
            "dependencies": [],
            "details": "• Create dashboard/app.py with placeholder layout (sidebar + main).\n• Configure Streamlit in .streamlit/config.toml (theme, page title, wide layout).\n• Update Dockerfile to install streamlit and expose port 8501; add CMD \"streamlit run dashboard/app.py --server.port 8501 --server.address 0.0.0.0\".\n• Verify local run: `streamlit run dashboard/app.py` renders empty page.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement cached data layer functions",
            "description": "Write SQLAlchemy-based functions (wrapped in st.cache_data) to fetch sentiments, prices, and news in required formats.",
            "dependencies": [
              "8.1"
            ],
            "details": "• Connect via SQLAlchemy engine from config.\n• Functions: get_buzzing_heatmap_data(start,end), get_sentiment_price_series(ticker,start,end), get_news_overlay(ticker,start,end).\n• Apply @st.cache_data(ttl=300).\n• Return pandas DataFrames ready for Plotly.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build Plotly heatmap for Top Buzzing Stocks",
            "description": "Render heatmap of post_cnt_zscore by ticker over selected period using Plotly in main page.",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "• Use get_buzzing_heatmap_data() output.\n• Create Plotly heatmap figure with color-scale diverging centered at 0.\n• Add hover showing post count, z-score.\n• Integrate figure into Streamlit container.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build sentiment vs price line chart with news overlay",
            "description": "Combine dual-axis line chart of sentiment average and price, overlaying news event markers.",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "• get_sentiment_price_series() returns sentiment and price columns.\n• Plotly secondary y-axis: left = sentiment, right = price.\n• get_news_overlay() provides vertical lines + tooltip headlines.\n• Add range slider & adaptive x-axis ticks.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add sidebar filters and state management",
            "description": "Implement date range picker, ticker multiselect, and internal session state to synchronize charts.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3",
              "8.4"
            ],
            "details": "• st.sidebar.date_input for start/end, st.sidebar.multiselect for tickers.\n• Use st.session_state to propagate selections.\n• Trigger data reload functions with selected parameters; ensure minimal reruns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Write Cypress end-to-end tests and verify container exposure",
            "description": "Automate E2E testing of dashboard rendering and performance, and confirm Docker port mapping.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3",
              "8.4",
              "8.5"
            ],
            "details": "• Add cypress/ directory with tests: visit '/', assert heatmap and line chart load within 1 s.\n• Use cy.request to validate backend API latency.\n• docker-compose override maps 8501:8501; GitHub Action runs `docker-compose up -d dashboard && npx cypress run`.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 9,
        "title": "Containerize Cron & Worker Services",
        "description": "Docker-compose orchestration of scraper, nlp, aggregator, alert, dashboard services.",
        "details": "• Extend docker-compose.yml: services: db, scraper, nlp, aggregator, alert, market, dashboard.\n• Use healthchecks. Networks internal.\n• environment variables via .env file.",
        "testStrategy": "`docker-compose up -d` then `docker-compose ps` shows all healthy. Logs show scheduled jobs executing. curl localhost:8501 returns HTTP 200.",
        "priority": "low",
        "dependencies": [
          3,
          4,
          5,
          6,
          7,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Service Dockerfiles / Base Image",
            "description": "Author individual Dockerfiles or a shared base image for scraper, nlp, aggregator, alert, market, dashboard services.",
            "dependencies": [],
            "details": "• Decide between shared Python base image vs per-service Dockerfile.\n• Install OS libs (curl, git, cron) and Python 3.11 + poetry/requirements.\n• Copy service code, install dependencies, set non-root user.\n• Output images tagged nssm/<service>:latest for use in compose.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Extend docker-compose.yml with Services & Networks",
            "description": "Add all worker services plus db to docker-compose and define internal networks.",
            "dependencies": [
              "9.1"
            ],
            "details": "• Define services: db, scraper, nlp, aggregator, alert, market, dashboard.\n• Reference images built in 9.1 or build context paths.\n• Create internal network \"nssm_net\"; attach all containers.\n• Mount volumes for persistent db data and logs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Healthchecks & Environment Variables",
            "description": "Introduce per-service healthcheck commands and centralised .env configuration.",
            "dependencies": [
              "9.2"
            ],
            "details": "• Add healthcheck sections (curl /health, pg_isready, etc.) with suitable intervals/timeouts.\n• Create .env file holding DB creds, API keys, scheduler intervals.\n• Map environment: key=value or env_file in compose.\n• Fail containers on unhealthy status for easier orchestration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Entrypoint Scripts & Cron Scheduling",
            "description": "Embed cron or schedule triggers inside containers to run workers at desired intervals.",
            "dependencies": [
              "9.2",
              "9.3"
            ],
            "details": "• Write entrypoint.sh per service: migrate DB, start cron or python scheduler.\n• Use crontab files or schedule lib; e.g., scraper every minute, aggregator hourly.\n• Ensure logs stream to stdout for docker-compose logging.\n• Update Dockerfiles (9.1) to COPY entrypoint & set CMD.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Local Orchestration Verification & Documentation",
            "description": "Test full stack with docker-compose and write README usage instructions.",
            "dependencies": [
              "9.3",
              "9.4"
            ],
            "details": "• Run `docker-compose up -d`; verify `docker-compose ps` shows healthy.\n• Tail logs to confirm cron jobs trigger and DB writes occur.\n• Hit dashboard on localhost:8501, expect HTTP 200.\n• Document setup, env vars, common commands, troubleshooting.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "Automated Testing & CI Pipeline",
        "description": "Configure GitHub Actions pipeline for linting, tests, build images.",
        "details": "• .github/workflows/ci.yml: matrix python 3.10/3.11.\n• Steps: checkout, setup-python, cache pip, install, run flake8 & black --check, pytest, build docker-compose build.\n• Push images to GHCR for main branch.",
        "testStrategy": "Push PR with failing lint → action fails. Fix → action passes and images appear in registry.",
        "priority": "low",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Base GitHub Actions Workflow",
            "description": "Set up a ci.yml workflow with Python version matrix and pip caching.",
            "dependencies": [],
            "details": "• Path: .github/workflows/ci.yml\n• Trigger: pull_request + push\n• Strategy.matrix: python-version [3.10, 3.11]\n• Steps: actions/checkout, actions/setup-python (uses matrix), actions/cache (key: v1-${{ runner.os }}-${{ matrix.python-version }}-pip), pip install -r requirements.txt\n• Verify workflow syntax via act locally.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate Linting Checks",
            "description": "Add black, isort, and flake8 steps that fail on style violations.",
            "dependencies": [
              "10.1"
            ],
            "details": "• Extend ci.yml with pip install 'black==23.*' 'isort==5.*' 'flake8==6.*'\n• Run commands: black --check . ; isort --check-only . ; flake8 .\n• Annotate failures using github-sane-report or default annotations.\n• Ensure non-zero exit stops job.\n• Update README with badge for build status.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Run Pytest with Postgres Service",
            "description": "Configure workflow to spin up Postgres container and execute pytest suite.",
            "dependencies": [
              "10.2"
            ],
            "details": "• Add services.postgres in ci.yml (image: postgres:15, env: POSTGRES_PASSWORD, ports)\n• Wait for DB readiness using health-check or bash retry loop.\n• Install test deps via pip install -r requirements-test.txt\n• Command: pytest -q --maxfail=1 --disable-warnings\n• Artifacts: upload coverage.xml and junit.xml.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build & Push Docker Images to GHCR",
            "description": "On main branch, build multi-arch Docker image and push to GitHub Container Registry.",
            "dependencies": [
              "10.3"
            ],
            "details": "• Add job conditioned on if: github.ref == 'refs/heads/main'\n• Steps: docker/setup-buildx-action, docker/login-action (registry: ghcr.io, using GITHUB_TOKEN), docker/metadata-action for tags (sha, latest), docker/build-push-action with push: true\n• Optionally cache layers.\n• Verify image appears under ghcr.io/org/repo.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-26T13:47:28.377Z",
      "updated": "2025-08-27T12:27:04.075Z",
      "description": "Tasks for master context"
    }
  }
}